,question,answer
0,Has anyone used Erlang or Elixir in production? How does it compare to Python or Go?," U02U6J2HZ2P has mentioned that Erlang is well established in telco which are pretty critical systems, and U015BH45ZK6 has mentioned that WhatsApp uses Erlang and that their claims about scalability are ""crazy"". U01FHM4NGDP has also mentioned that Erlang is good for zero-downtime upgrades and abstractions that let a data scientist directly author data pipelines."
1,Is Object Oriented Programming (OOP) terrible when writing software that deals with huge data and has to be performant in complex cloud-native setups? What do people exactly mean when they refer to 'OOP' and 'OOP developers'?," No, Object Oriented Programming (OOP) is not necessarily terrible when writing software that deals with huge data and has to be performant in complex cloud-native setups. When people refer to 'OOP' and 'OOP developers', they usually mean adhering and utilizing the four principles of OOP: encapsulation, abstraction, inheritance, and polymorphism."
2,"How do you track inferences over time for ML observability and what tools do you suggest to use for this? Do these tools support unstructured data like images, voice, and text?"," For tracking inferences over time for ML observability, one could use tools such as WhyLabs, Arize AI, Evidently, MLFlow, Tensorboard, and Alibi Detect. These tools may support unstructured data like images, voice, and text, depending on the particular tool; it would be best to check the features of each tool to see if it would meet one's needs."
3,"Is there a blog or repo with code for a simple model built and deployed in Sagemaker, Kubeflow, and MetaFlow? Any insights on DX differences? Any comparable refarchs available? Any suggestions on good use cases where these tools excel? "," There is a repo on GitHub for deploying to Sagemaker with BentoML: <https://github.com/bentoml/aws-sagemaker-deploy>, as well as a helpful comparison of how to write a very simple workflow using a few different authoring paradigms found in the Ray Project's GitHub repository <https://github.com/ray-project/ray/tree/7d1f408de966f90c95efe5b0cf488e82cd1c6e17/python/ray/workflow/examples/comparisons>. Additionally, there is a blog post and a TDS series available at <https://github.com/jacopotagliabue/post-modern-stack> and <https://towardsdatascience.com/the-post-modern-stack-993ec3b044c1>, respectively. There is also a 3-part series written by <@U01R2314L02> digging into ZenML found at <https://www.fuzzylabs.ai/blog-post/the-road-to-zen-serving-models>. For tools to help with data and model versioning, Pachyderm"
4,"What are some best practices or open source frameworks for caching pre-computed ML predictions, especially for high cardinality use cases that require invalidation and re-triggering on a regular basis?"," Lifecycle policies in S3 (or any other object storage service) can be used to keep data for different periods of time for each client. Other alternatives include using MongoDB, Feast, BigQuery + Firestore, or DynamoDB."
5,What are some examples of ML tooling companies with good documentation?," MLFlow, FastAPI, Django, PyTorch, Diataxis, MLEM, Wallaroo, and Ray."
6,What are the opinions of MLOps.community Slack channel members on TensorFlow Extended (TFX) for model development and productionization and how does it compare to other options?," The members of the MLOps.community Slack channel generally agree that TFX is a good choice if you are heavily invested in the TF ecosystem, but for most people the developer experience is not great and other Cloud provider solutions such as SageMaker, Vertex AI, and Azure ML are better options. There is also a general preference for PyTorch over TensorFlow, as PyTorch offers more model availability and the ability to export to 3rd party formats."
7,"What are the best practices for batch model & online prediction architecture, and what approach should be used for scalable API?"," It is recommended to use a scalable endpoint approach, with request batching, a dedicated endpoint for batch jobs, and a service, deployment, and HPA for auto-scaling. Additionally, using a model or artifact store and standardizing deployment patterns to as few as possible can help with scalability. It is also recommended to use a dashboard and alerting system for managing many models."
8,How do people manage different versioning schemes on their inference services?," People manage different versioning schemes on their inference services by using semantic versioning, a short git sha, and a timestamp. They can also use dummy commits/PRs into their CI flow and remove the semantic versioning from the versioning scheme. Additionally, they can use nvidia-docker to manage different versions of tensorflow/pytorch dependencies around cuda & cudnn locally on their DGX machine, and they can define the version (runtime) of the model server they’d like to have in the inferenceservice spec."
9,How do you determine whether or not you are able to reproduce models?," You can compare the model files using a hashsum, compare all weights and biases and layers of the model, or log all hyperparameters that goes into your framework."
10,What were the best MLOps conferences/events attended this year and why?," People mentioned the MLOps Salon events put on by Verta, the Dask Distributed Summit, ODSC, MLOps World, MLOps Community streaming platform, MLOps Conference, and MLCon by cnvrg. Reasons given included getting external speakers to talk about real-world use cases, high-quality speakers talking about practical examples, and relaxing their policy against corporate speech."
11,What are the challenges faced and recommendations for alternatives for building an end-to-end MLOps engine with Kubeflow and AWS?," The challenge with building an end-to-end MLOps engine with Kubeflow and AWS is that Kubeflow is made up of multiple interconnected pieces with varying levels of maturity and development. The uncertainty of various pieces of the system, such as MLDL, makes it difficult to use long-term effectively without an incredibly capable k8s and MLOps infra team. Alternatives could include Seldon for model serving, Determined, ClearML, Airflow, Pachyderm, DVC, MLFlow, Kale on Jupyter Notebooks, Kserve, AWS Services, Argo Workflows, and the AI Infrastructure Alliance."
12,"Are there any published benchmarks for the compute (cpu/gpu) needed by trained models in production such as spacy, mt5, and bert?"," Yes, MLCommons has training and inferencing benchmarks they run, including for BERT."
13,"When deploying Tensorflow models, is there a reason not to convert to TFLite and use their runtime? Is it faster even if not using edge devices?"," It is possible to make a lighter image and potentially run things faster even if not using an edge device, although the TF Lite comes with a subset of operators compared with the full version. Additionally, there is a chance of performance improvement when using ONNX conversion."
14,What are the best practices for version controlling Jupyter Notebooks?," The best practices for version controlling Jupyter Notebooks include using nbdime for version control, papermill for parameterizing and running notebooks like a script, writing normal packages that can be version controlled and importing the functions from those packages into notebooks, using jupytext to open .py files as notebooks, and treating notebooks as temporal and the script versioned in github as the source of truth."
15,Are there any ML/MLOps/Engineering summer schools that are engineer focused for people that are not in academia? ," Yes, there is Full-stack Deep Learning: <https://fullstackdeeplearning.com/>. It's a hands-on, less-theoretical course on deep learning, including the ops stuff _around_ model building and deployment. It was last held in autumn 2022 and was $400, though they offered some scholarships if that price point was out of one’s budget."
16,"What are the best practices for logging, monitoring, and alerting with microservices and what are the critical things to log, monitor, and alert for initial deployment of microservices?"," Best practices for logging, monitoring, and alerting with microservices include standardizing metrics and log formats in a language-agnostic way, logging everything you want to set metrics and alerts on, and logging model inputs and outputs to assess accuracy later. Critical things to log, monitor, and alert for initial deployment of microservices include failure points in dependent services and user interaction with edge-deployed models."
17,"Has anyone written or come across detailed lists of what MLOps engineer would require? Model deployment is a broad category, and we’re looking for something more targeted, perhaps inference resource management."," U017CCHCW8K has compiled the MLE curriculum in a Google Document, and U02UDUTS919 suggested an article by Google Cloud which lays out the step-by-step process of bringing MLOps."
18,"Is pyspark/spark still necessary for analytics on big data or are there alternative tools like Redshift, Snowflake and BigQuery? What are the downsides of other solutions compared to spark?"," Spark is still necessary for many use cases, such as stream processing and really big data pipelines. Dask and Ray are alternatives, but they can be too brittle and immature. Spark has an advantage of being well known and adopted, so it's easier to find answers and hire people to support it. Other solutions like Redshift and BigQuery can be better for data transformation, but they aren't suitable for long running jobs or complex transformations."
19,Is it feasible to use DVC as a data/model repo and stream models into memory for microservices in k8s? Is there a recommended way to fetch the latest committed model in PR?," Yes, it is feasible to use DVC as a data/model repo and stream models into memory for microservices in k8s. There are a few ways to fetch the latest committed model in PR, such as using `dvc get`, `dvc.api`, or `git clone` + `dvc pull mode.plk`."
20,What are some other signals worth watching regarding the pullback on funding for Cloud ML products and where does Pitchbook get their data from?, I don't know.
21,"When working with large image datasets for computer vision, what are the benefits and drawbacks of running augmentations in memory vs. in a pipeline with the results stored in S3?"," The major benefits for computing beforehand and storing are:
• Extremly fast during training
• Reduced computational load (less RAM/CPU/GPU power required)
The main drawbacks there are:
• May need a lot of storage
• Only N fixed versions per sample
For online computation it’s the other way around.
Pro:
• A lot more potential diversity (especially when combining many transforms)
• No need to store them all (when combining many transforms, you potentially have a huge N -&gt; Need a lot of storage (~Nx your dataset)
Con:
• Takes significantly longer (especially if your transforms are costly. E.g. medical image computing with 3D image interpolation is very costly, whereas 2D transforms can be better parallelized and if applied with sufficient processes are only a minor bottleneck if at all)
• Higher ressource demand (except for storage ofc)"
22,What are some good resources for scaling inference workloads on multiple GPUs using Kubernetes clusters or other solutions?," Search for ray, deepspeed, hugging face accelerate, Horovod, and Efficiently Scaling Transformer Inference (https://arxiv.org/pdf/1807.05358.pdf). Additionally, Nvidia Triton Servers and Runai Atlas may be helpful."
23,What are the pros and cons of using Dask to run data pipelines for ML compared to using Spark?," The pros of using Dask to run data pipelines for ML compared to using Spark are that it is more easily usable for the target ML scientist, has a lower learning curve, is easier to troubleshoot and tune, and is more software developer friendly with branching, local development, and debugging. The cons of using Dask to run data pipelines for ML compared to using Spark are that it is not as well known as Spark, it may be harder to find answers to complex errors, and it may not be as well suited to long running jobs or complex transformations."
24,What's the general way to answer if a model can run with a particular runtime?," Docker-based deployments cover most use cases, since you can bundle pretty much whatever runtime you want in the image."
25,Has anyone set up distributed multi-node GPU training in Pytorch using torch elastic? Are there any resources or performance benchmark analysis available? Any insights on using Horovod?, U020ZV0UHL5 has set up distributed multi-node GPU training in Pytorch using torch elastic and has shared a blog post about it. They have also shared insights on using Horovod. U020ZV0UHL5 has also published a docker image and shared a repo URL. There are no specific resources or performance benchmark analysis available.
26,Is there an open-source platform/tool that allows defining on-demand scoring flows?," Seldon Core has the concept of inference graphs allowing you to write custom inference logic including splititng out pre-processing and post-processing, ensembling models, routing etc."
27,Are successful DS/MLEs expected to be more proficient with coding and ops than normal SWEs?," It depends. Some ML code is simpler than SWE code, but there are also areas such as concurrency, fault tolerance, and system architecture where SWEs may have more experience. Ultimately, the proficiency of DS/MLEs and SWEs can vary depending on the situation."
28,What model serving tools would you recommend for quickly deploying multiple ML models into production with flexibility for custom preprocessing?," KfServing (part of KubeFlow), TensorFlow Serving Server (part of TensorFlow), AWS Lambda, GCP Cloud Run or Cloud Functions, Azure Functions, Algorithmia, and Huggingface."
29,"What are the experiences and thoughts of users heavily using or evaluating AWS SageMaker, and what tools have they found useful and where did they find value? Are there any gaps or issues requiring tools outside of the AWS ecosystem?"," Users heavily using or evaluating AWS SageMaker have found custom algorithm training to be annoying, experiment tracking not built in, documentation to be decent but hit or miss, endpoint inference to be clunky based on ECS running on EC2 with servers running 24x7x365 with load balancing using ASG, and deployment to be basic with nothing fancy in terms of graphs or deployment patterns. They have also noted that there is no data lineage tracking like DVC and model lineage is built but model registry is not existent. Additionally, they have found working with custom script to be a nightmare and that users need to do a huge amount of testing before invoking the estimator or else they will not get results but still pay the billable seconds. Gaps or issues that require tools outside of the AWS ecosystem include experiment tracking, data lineage tracking, and model registry."
30,"Has anyone used serverless GPU providers like banana.dev, slai.io or any other serverless GPU provider for production use cases? Would like to connect/chat to understand more about how you are using it."," People have mentioned Paperspace and LambdaLabs, and GenesisCloud has been mentioned as well. People have also used big cloud providers (AWS, GCP) and Colab for prototyping."
31,"What options are available for improving a machine vision application's generic pipeline consisting of image retrieval from cameras, running algorithms, storing images, sending data to a visualization platform, etc.?"," Options for improving a machine vision application's generic pipeline include using FastAPI, dockerizing tasks, using profilers to pinpoint problem areas, reorganizing the base image to reduce build time, and using Docker stats for resource monitoring."
32,What should be the standard practice when deploying your model weights with Docker and what are the common mistakes people make when downloading or baking their model weights?, The standard practice when deploying model weights with Docker is to bake them into the Docker image and/or a Python package. Common mistakes people make when downloading or baking their model weights include not using a standard way of packaging software for distribution and not having a versioned way to keep track of the models being used.
33,What are some preferred methods for converting PyTorch models to TensorRT for deployment using Nvidia Triton? Are there any issues with using TensorRT for dynamic batching?," Some preferred methods for converting PyTorch models to TensorRT for deployment using Nvidia Triton include compiling models using torchscript and using the PyTorch backend, and converting PyTorch models to ONNX and then to TensorRT. There can be issues with using TensorRT for dynamic batching, such as when trying to compile models that support dynamic batching."
34,How to resolve the issue of a pod remaining unschedulable despite adding tolerations?, Try installing the drivers as suggested in the Google Kubernetes Engine documentation.
35,What would be the checklist for taking over an ML project for maintenance or further enhance the model between two teams with similar skill sets?," A checklist for taking over an ML project for maintenance or further enhance the model between two teams with similar skill sets could include: meeting and decision minutes (mainly rationalities of the decisions), code and data lineage, howtos, reports and how they are created, automated tests, model code that has been reviewed and approved by at least one other developer, code that passed the ""can be understood without comments and doc-string"" test, existence of code review guidelines, model versioning and metadata tracking, and any items that could improve the reliability of the ML system."
36,"What is the best solution to manage multiple models in a system pipeline, and how to handle dynamic mapping of models when using RESTful containers?"," You can create a service that acts as a reverse proxy, which is responsible for starting the docker machines dynamically and generating the addresses. You can use an ingress to route the requests to different models, and update the ingress rules manually, automatically using pre start and post start hooks, or using a CRD. Kubeflow or a similar platform may also be useful, as their APIs largely solve the ""wiring"" problem."
37,Has anyone worked with AWS EKS for managing ML pipelines in combination with AWS SageMaker and CI/CD? ," Yes, U01T8RK505N has some experience on this front."
38,What is the preferred way to deploy multiclass classifiers?," It depends on how many classes there are, whether we care about all of them in prod, and whether “none of these” needs to be a viable response. Hierarchical Parent Node + Soft Voting is one approach."
39,Is the 'operations' tag in MLOps limiting? How do people typically define MLOps? ," The 'operations' tag in MLOps can be limiting, as it can imply a focus on ""keeping the lights on"" rather than innovation and experimentation. People typically define MLOps as a cross-functional process aiming to improve AI/ML products lifecycles and involving people with different backgrounds. It is also important to note that MLOps is related to provenance and is larger than DevOps."
40,Is it possible to install two versions of the same pip packages to the same python environment and choose which to load? What's the least hacky workaround?," It is possible to install two versions of the same pip packages to the same python environment and choose which to load, but it is a hacky workaround. One suggestion is to define optional dependencies on the package, update the model class to handle versioning logic, or use a notebook to pip install the relevant version before loading/evaluation. Another suggestion is to use setuptools-scm for git based versioning, and use git tags for specifying releases."
41,"What resources are available to help train an ML model more effectively, specifically an XGBoost model on historical data that is performing poorly? How can one avoid trial and error while retraining the model, and is there a way to understand what is causing the performance issues?"," A few resources that could be helpful include: a guide to better navigate XGBoost parameters and ranges (<https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/>); Google's Rules of ML (<https://developers.google.com/machine-learning/guides/rules-of-ml/>); a blog post on making ML work (<https://www.shreya-shankar.com/making-ml-work/>); and a tool for adding real-time personalization to ranking items (<https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/>). Additionally, one should check their data processing for errors that could be destroying the structure of the data, as well as inspecting and exploring the dataset to check for other problems. SHAP values (<https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Basic%20SHAP%20Interaction%20Value%20Example%20in%20X"
42,What are some recommended open source visualization tools for EDA?," Bokeh, d3.js, geoplotlib, perspective, altair, seaborn, pandas-profiling, Apache Superset, Metabase, and What-If Tool."
43,"As the only data scientist for an ecommerce startup, what can I do? Any suggestions?"," There's a ton you could do. I would start with the basics for internal needs. Things like all the data aggregated and make sure you understand it. Start generating simple descriptive analysis, ask the business people what will add value to them. Then identify with the business units what would add value in terms of predictive analysis like analyzing the behavior of creators that generate the most revenue, what are their characteristics, what do they do in the platform, etc. try to figure out how to retain and bring in more of those people and try to figure out how to prevent they leave. In principle more that trying to think what you can do by yourself I would reach out to the business units and try to understand what would make a difference to them and try to prioritize. Keep it simple at this point."
44,When should we use Prometheus Alertmanager instead of configuring alerts in Grafana when using Prometheus and Grafana together?," It is considered best practice to use Prometheus Alertmanager over Grafana's alert system, assuming you don't have any other sources of alert than Prometheus. This is because Grafana is not as good in deduplicating alerts, especially if you have it configured with high availability, and if the connection between Grafana and Prometheus fails, you suddenly have no alerts."
45,What is the best tool for env management of python in 2023?," It is difficult to say, but some popular tools include Poetry, Pyenv, Venv, Conda, Pip, Mamba, Fastchan, and Dynaconf."
46,"What are some strategies for incorporating more recent data into an ML model, retraining it, and validating that it has better performance than the existing model, without biased evaluation?"," Strategies include keeping track of exact samples included in the validation and training sets, using a time-dependent K-fold train/test strategy, and using A/B tests. Other strategies mentioned include splitting the data by time, using early stopping, and retraining on a regular interval."
47,Does anyone else have a platform for making sure data stays private and secure?," U015BH45ZK6: Ugh, we (Hypergolic) made the product plans for an MLOps tool for just this usecase for a major global player who wanted to enter the space. U017CCSNYDV: Once you give someone access it is very difficult to stop them transporting that data. You might consider combining firewall with DLP (<https://cloud.google.com/dlp/docs/concepts-infotypes>) to get towards what you need. I think synthetic is the way to go. DP is something to build in too (<https://github.com/IBM/differential-privacy-libra>)."
48,What are some methods and resources for building dynamic pricing models in short term rentals and how can demand be factored into these models?," U03NHMDHRQW mentioned using a markov decision process with a definite end state to create a dynamic pricing model in the short-term rental context. U03DR0BQR50 suggested random variation in prices to help with identifying price elasticity and modeling a demand curve. Additionally, U01N8ERHH9T and U02PUD51UGH mentioned multi-armed bandits and GBMs, respectively, as methods for dynamic pricing. U02PUD51UGH also recommended a KDD paper (https://arxiv.org/abs/2207.01137) and Alibaba/TMall papers as resources for more information."
49,How can I access the recorded version of the LLMs for the production conference held on the 13th of April 2023?, UP3T8K9M5 has mentioned that the recording will be coming soon.
50,"When building/evaluating a framework for ML experimentation, training pipelines, do we bake off kubeflow/mlflow/weights and biases or build our own software stack that just abstracts those implementations? Has anybody released to OSS their own code stack?"," We open sourced our stack at Netflix - it essentially abstracts the fundamental components of ML infra -data storage, compute, orchestration, experiment management etc. (<@U01BNPM89U1> which Netflix project was that? Metaflow? <https://github.com/Netflix/metaflow>)"
51,"What are some options for packaging models without separating them from their underlying runtime, and has anyone had experience with using ONNX?"," Options include packaging everything as a docker container and using ONNX or another serialization format. People have had experience with ONNX, but there have been some bumps along the way related to versions and expanding it to GNN library primitives."
52,"How do you deal with access given to Data Scientists and ML Engineers/SE in a model registry, specifically with regards to S3, and what are some proposed solutions?", One way would be to have a single registry where the DS will push the model to and MLEs can pull the model from. Another proposed solution is to host an MLflow tracking server in one account and use the mlflow python API to pull the models without S3 access. Access controls and authentication can be managed by the ML team by giving everyone admin rights or setting different levels of access.
53,What is a good experiment management tool for non-production purposes with the ability to scale GPU on demand and compare experiment results?," Kubeflow-based experiment tracking platforms may work well for this purpose, as well as cloud platforms that provide tuning services that rely on bayesian sampling to optimize model training. Additionally, tools such as Raytune and NNI may be good options."
54,What are the downsides of models in container images and what alternatives are there?," The downside of models in container images is that you have to build an image `n` times for a release x `m_n` models per client, which can become a bit annoying to manage. An alternative is to use object storage (minio for on-prem) and push the models there (mirroring a bucket from internal object storage)."
55,What are the best practices for multi-tenant MLOps flow in Databricks? How can multi-tenant models be trained in Databricks?," It depends on your use case and data isolation requirements. There are various options for orchestrating the jobs, such as using multi-task jobs and the Databricks API, or using an external linked service such as Azure Data Factory or AWS Step Function plus Lambda. You can also consider using a tool/product that helps you isolate each tenant with a notion of projects, which isolates entities and manages access rights. Additionally, you can use AutoML solutions to account for variation in features across tenants."
56,What is the experience with using synthetic data in a production-ready machine learning workflow and how to deal with errors in the data?," U015BH45ZK6 mentioned that they heavily used synthetic data for their NER/NEL system, which included heuristics, partially labelled data, and majority voting by other lower quality NER systems. To deal with errors in the data, U015BH45ZK6 suggested that the first step is to start converging offline data towards what is in prod, and measure differences and figure out why that happens and if it significantly influences model performance. Additionally, U016624NW3U suggested that the only data that should be used for data projects is production data, and U01JFBUKMCG suggested testing in prod by doing a canary release or shadow traffic to new models."
57,Is ML part of the apps and tools we buy or do companies also try to do ML in-house?, Both. Companies will try to do ML in-house and also use tools that leverage ML.
58,How do you operationalize training a model back and forth between notebooks and an automated pipeline?," There is no one-size fits all pattern for operationalizing training a model back and forth between notebooks and an automated pipeline. Some guidelines include maintaining code and requirements.txt in GitHub, externalizing the configuration, building a Docker image and pipelines, doing a dry run of the end to end pipeline, and deploying the pipeline to production with alerts and monitoring."
59,What meeting framework or outline would be recommended for early discussions on developing better MLOps practices? ," A suggested framework for early discussions on developing better MLOps practices could include focusing on the problems that need to be solved, the effort and value of the solutions, and deciding as a group on the MLOps maturity model. It could also include getting buy-in from the team, using the SCAR framework, and providing an introduction to MLOps, best practices, and how it differs from DevOps."
60,How are other people handling the 'batch in disguise' problem which involves requests coming in for individual keys/inputs with a third-party marketing system that is largely outside of one's control?," Monzo has an in-house experiment system with assignment of variants, web UIs to start and pause experiments, and Looker templates for metrics and significance calculations. There is no magic solution, but building strong relationships with leaders in the company is important. Additionally, it helps to look for problems that can be solved with classification or regression and look at the key metrics related to those problems."
61,What are the things to be learnt in MLops for someone new to it?," Things to be learnt in MLops for someone new to it include how to write YAML files, model deployment, various technologies related to MLops, Data Engineering knowledge, and MLOps principles and best practices."
62,How does one deal with refactoring and maintainability in data science when it's more dynamic and iterative than software development?," It depends on the company/product and scale, but often it does not require huge refactoring and maintainability efforts. However, there may be a need for bigger refactoring efforts when it comes to the initial exploration phase of a data science problem. It is also important to consider the balance between experimenting and operationalizing, and to avoid the ""Gold plating effect"", while calculating ROI and managing dependencies."
63,"What tools can be recommended for a team to unify research and MLOps development teams, for quick deployment and management of different ML models with versioning?"," MLFlow, Verta.ai, and Whylogs can be recommended for a team to unify research and MLOps development teams, for quick deployment and management of different ML models with versioning. Additionally, DVC can be used for large file tracking and versioning, and GitHub Projects can be used for project management."
64,How do I integrate MLflow in ZenML for experiment tracking?, There is a good blog by <@U029FHM5LPP> from ZenML on this <https://dev.to/zenml/how-to-improve-your-experimentation-workflows-with-mlflow-tracking-and-zenml-2dhe> and we wrote about it recently as well: <https://www.fuzzylabs.ai/blog-post/the-road-to-zen-running-experiments>.
65,Is it possible to have multiple containers share a single GPU with AWS ECS Agent updates?," No, it is not possible to have multiple containers share a single GPU with AWS ECS Agent updates."
66,What decision framework can be used to choose a specific ML tool from the countless options available?," I recommend having 2 phases:
• Phase 1: Create a short list of potential candidates (e.g. 2 to 5 items). This can be reviewed just by looking at the product features and online reviews (low time investment, but a lot of solutions).
• Phase 2: Evaluate each candidate during a PoC to assess their pros and cons (high time investment, but fewer solutions).
Before each phase, you should come up with a list of criteria to evaluate the solutions objectively. It's better if you can involve end users to confirm the criteria and weigh their importance."
67,"What are some good resources with examples, preferably with diagrams, that show how other companies have structured their data architecture and task orchestration ecosystems?"," U01HBEHNX16 suggested the GitHub repository <https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat> and the book <https://www.amazon.com/Informed-Company-Cloud-Based-Explore-Understand/dp/1119748003>, U015BH45ZK6 suggested getting in touch if interested in a chat, U019ABXBYET suggested the blog post <https://monzo.com/blog/2022/04/26/monzos-machine-learning-stack>, and U01J0NVNE1G suggested the Google and Microsoft architecture examples."
68,Are RNNs and LSTMs still used for NLP tasks or has Transformers taken over? Are these models productionized and how much resources are required monetarily?," Transformers are the most popular for NLP tasks in research, but they are not always needed in industry and lots of production systems still rely on ARIMA or Prophet from Facebook. It really depends on factors like the ML team, the difficulty of deploying new models, and how important the ML models are to the company. The cost of fine-tuning a transformer depends on factors such as the architecture, training data, number of training epochs, and hardware available."
69,Is it possible to get two metrics from two runs into the same plot in Neptune?," No, it is not possible to get two metrics from two runs into the same plot in Neptune."
70,"What are the other options available to migrate from training on demand architecture, and can Celery be used to build task queues instead of handling requests on gRPC server threadpool? Also, what are the ways to handle GIL issues? ", Other options available to migrate from training on demand architecture include using a queue system or designing a tool that adds training data to the database layer and polling this layer instead of training whenever a request is received. Celery can be used to build task queues instead of handling requests on gRPC server threadpool. Possible ways to handle GIL issues include using a multi-process approach to handle multiple requests simultaneously or using an asynchronous programming model.
71,"What are the recommended experiment tracking libraries for a simple application with input parameters, and tie it to a resulting model, and is MLFlow the best option available?"," MLFlow is recommended for this application and is the most popular and familiar option for collaboration. The only cons are the additional management and lack of authentication/workspace if you want to scale the solution. Alternative solutions might include Sacred, W&B, Neptune, Comet, or ClearML."
72,What do people use for renting cloud GPUs?," People have used AWS, GCP, Paperspace, LambdaLabs, and GenesisCloud."
73,What is Vertex Feature Store and is it based on Feast?, Vertex Feature Store is a feature store from Google and it is not based on Feast.
74,"Is there a model registry that can accommodate models with a combination of weights, architecture, and code for pre- and post-processing? How can MLFLow artifacts be utilized with the MLFlow model registry?"," Yes, MLFlow can accommodate models with a combination of weights, architecture, and code for pre- and post-processing. MLFlow artifacts can be used with the MLFlow model registry by tracking the model metrics, plots, auto logging from mlflow, and code (the seldon folder) together and then registering that run as a model. All artifacts are stored on the model registry (the model registry is just a pointer to a experiment run)."
75,What is the best practice for running FastAPI services in k8s/ECS/etc and how should uvicorn workers and containers be configured for optimal performance?," According to good practices, there should be one container per “atomic” process and the number of uvicorn workers should be matched to the number of CPU processes available. However, you may get better performance by running fewer but larger containers instead of many small containers. Additionally, you should try removing the CPU limit when running your FastAPI services, but setting the CPU requests."
76,What framework did the guys from Mage use to build their UI and did they have a designer? ," They used React for the components, wrote in TypeScript, and used NextJS for the framework. They also had an in-house designer and invested time and money in the UI and UX."
77,How can we communicate about ML without complicated things?," We can use plain English to communicate about ML, and focus on the benefits it can bring to the business. We can use analogies or demonstrative exercises to explain the concept, and talk about specific top of mind goals and business KPI's to make it relevant to the business."
78,What are people's thoughts on AutoML and are there successful use cases? How can AutoML models be improved upon? Are there any limitations to using AutoML in regulated industries?," People generally agree that AutoML can be useful for quickly benchmarking models and setting up automated pipelines. It can be dangerous if people are using it without understanding the assumptions models are making. There are successful use cases, particularly for low impact prediction tasks where there are not enough ""real ML people"" to do it. To improve AutoML models, techniques such as better hyperparameters, data augmentation, and feature engineering can be used. There can be limitations to using AutoML in regulated industries due to liability and indemnification concerns."
79,What are some resources or guidance for improving MLOps knowledge and skills specifically for computer vision models?," Some resources and guidance for improving MLOps knowledge and skills specifically for computer vision models include:
• <https://github.com/DataTalksClub/mlops-zoomcamp>
• <https://github.com/visenger/awesome-mlops>
• <https://ml-ops.org/>
• <https://madewithml.com/>
• <https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning>
• <https://developers.google.com/machine-learning/guides/rules-of-ml>
• <https://learn.microsoft.com/en-us/azure/architecture/example-scenario/mlops/mlops-technical-paper>
• The <#C01DEPTQ0F5|learning-resources> channel."
80,What is the best way to give a heads up to community to come and attend a Kubeflow event hosted by a company in New York City?," The best way to give a heads up to the community to come and attend a Kubeflow event hosted by a company in New York City is to post the event details and a link to the event registration on social media. Additionally, providing a list of speakers and a description of the event can help to generate interest."
81,What are some options for integrating a Python-trained ML model into a Matlab workflow for deployment on a rapid prototyping machine?," Options for integrating a Python-trained ML model into a Matlab workflow for deployment on a rapid prototyping machine include training the model in Python and then exporting the trained model into C code and importing that code into a Matlab S-Function, or doing the ML work in Matlab and doing most of the iterative work on Python and then the last bit of tuning/final training on Matlab."
82,"Are there many people in the MLOps community who work as independent contractors and/or work only four days a week, and is it possible to find work with a better work-life balance than corporate jobs in the field of MLOps?"," Yes, there are many people in the MLOps community who work as independent contractors and/or work only four days a week, and it is possible to find work with a better work-life balance than corporate jobs in the field of MLOps."
83,How can I get kfserving to work properly on a private cluster?," You may need to create an internal load balancer for the knative gateway to function properly. You may also need to create a firewall rule to allow ingress on port 6443 from the master IP range, as well as port 8443."
84,What are the experiences of using Ray/Anyscale and how seamless is the shift from research to productization? Has anyone used Ray in Computer Vision space?," People have used Ray for distributed computation, model serving, and batch predictions. Anyscale provides an application development platform for developers to build distributed applications and offers standardized platform for distributed computing. As for the shift from research to productization, users have found the abstractions to be great but noted that it requires the installation of the Ray infrastructure in the background. Regarding use in the computer vision space, one user noted that it was promising but not mature enough for prod use, and the other user was planning to revisit it as improvements have been made."
85,"What is the most effective way to do scalable real-time inference of custom models in AWS? Specifically, between using Lambda to call a model hosted on SageMaker or calling an ELB that routes to an autoscaling ECS service hosted on Fargate?"," It depends on the complexity of the model and the latency, hardware, and payload size requirements. Lambda can be used for smaller loads, but for larger loads, an ECS Service with an ELB in front may be a better option. For ultra-low latency requirements, SageMaker Real-Time Endpoints may be necessary."
86,Which feature store solutions offer feature versioning? Can AWS Sagemaker FS offer feature versioning? How does SMFS eliminate the need for Glue jobs?," Feature Form offers feature versioning with something called variants. AWS Sagemaker FS does not offer feature versioning, but it does save every historical version of each feature in the offline store, so you can do point-in-time dataset recreation. SMFS eliminates the need for Glue jobs by storing the offline store in S3 and automatically building a Glue data catalog for it."
87,"Is there a link provided for virtual workshops in the mlops world conference? If so, where can it be found?"," Yes, all info for virtual workshops can be found in the engineeringlabs channel and on the newsletter at https://go.mlops.community/newsletter."
88,"What MLOps technologies and projects would be recommended for deploying PyTorch models to Kubernetes containers for non-trivial orchestration needs involving monitoring drift, automatic retraining, data validation, and model and data versioning? Additionally, what distributed training tools and orchestration and DAG tools would pair well with Seldon if automation of its deployment is desired?"," Seldon, Pachyderm, ClearML, Kserve, DVC, Kubeflow, MLFlow, Airflow, Argo Workflows."
89,"What are some examples of effective data wrangling to model building pipelines for NLP preprocessing of clinical notes, cleaning/standardizing structured data, and training patient-level embeddings for downstream tasks?"," PyTorch's Datasets is a good example of an extensible way to handle data wrangling to model building pipelines for NLP preprocessing of clinical notes, cleaning/standardizing structured data, and training patient-level embeddings for downstream tasks. Ray was used in the past for heavy-duty ML pipelines and featured an in-memory object store to make sharing large objects across tasks easy. It is possible to make small changes to Python pipelines and have them be orchestrated by Ray. For Kubernetes support, Ray can be deployed on K8s."
90,What is the most common language used for developing computer vision models and how does deployment on edge devices or cloud affect the choice of language?," Python is the most common language used for developing computer vision models. Deployment on edge devices or the cloud may affect the choice of language, as some languages may be more suitable for use on certain platforms. For example, PyTorch models can be exported to CoreML for deployment on iPhones running Swift/C++/Objective-C. Additionally, there are model converters available for converting models trained in Python to the target platform/device with little to no loss in performance."
91,"What tools are available for data observability, monitoring drift in predictions, and overall data health before consumption into a feature store and/or models?"," Tools mentioned include elementary-lineage (open-source), Grafana + TICK (on-prem), BigQuery + Data Studio (for more involved data quality metrics and KPIs), come.ml, MLFlow, evidently.ai, alibi detect, TFMA, Seldon, AWS Sagemaker Model Monitor, BentoML, Fiddler.ai, Monalabs.io, and Kubeflow + TFX."
92,"When prototyping infrastructure on multiple local machines, should one use docker containers or VMs?"," Docker containers are an improvement over virtual machine images because they are more lightweight and quicker to experiment with. They are also better for scaling out and orchestration. For local development, however, virtual environments may be simpler."
93,"What are some recommendations for a small team trying to implement MLOps, specifically around toolset, Python programming, CI/deployment, project scaffolding, documentation, project registry, data exploration/preparation, tests, feature store, data versioning, experimenting, and training?"," Toolset: PyLint, Flake8, MyPy, Black, Docker, Kedro, Sphinx, GitHub Projects, Argo CD, Flux CD.
Python Programming: Visual Studio, remote interpreter, SVN repository.
CI/Deployment: Jenkins, Docker.
Project Scaffolding: Kedro template.
Documentation: Generate documentation from the projects, using Sphinx or a similar tool.
Project Registry: SVN repo with all projects as folders.
Data Exploration/Preparation: Matplotlib, Seaborn, Pandas, PySpark, Scala, SQL in Impala, Dask, Koalas, Vaex.
Tests: Unit testing may be too much burden, consider Great Expectations library.
Feature Store: Consider solutions like Tecton.
Data Versioning: Consider solutions like DVC.
Experimenting: Consider solutions like MLflow, ZenML, or ClearML.
Training: Consider solutions like BentoML, Clipper, or TFX."
94,How can data mesh and feature stores coexist? Should there be a centralized team to maintain a feature store with multiple data products or should a data product use a feature store and serve the online and historical data as different ways of consuming the features?," It depends on the system, but feature stores are usually designed as an interface of various data storages, sources, and domains, as a single API for fetching ML features, so it doesn't concentrate the storage. It is possible to route data from different data sources to a feature store, and some companies use different accounts for different contexts. Open source feature stores like Featureform and Feast are good options to consider."
95,"What is the best method to use for deployment in MLOps, MLflow + Seldon or Kubeflow? And can MLflow and Kubeflow or Seldon be used together for deployment?"," MLFlow + Seldon or Kubeflow can be used for deployment. Kubeflow can be used for training and deploying to Seldon. MLFlow can be used for a simpler approach to experiment tracking and deploying to managed platforms such as Amazon Sagemaker, and can also be used with Kubeflow."
