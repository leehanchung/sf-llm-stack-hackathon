{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"LkNZ6hp6K8l6"},"source":["# LLM Stack Hackathon (June 3, 2023) Starter Kit"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rTCDcPB9LMjx"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import json\n","\n","import openai\n","\n","from dotenv import load_dotenv\n","from IPython.display import HTML, display\n","import pandas as pd\n","from IPython.display import HTML, display\n","from tqdm import tqdm\n","import numpy as np\n","\n","import redis\n","from redis.commands.search.field import TagField, VectorField, TextField\n","from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n","from redis.commands.search.query import Query\n","\n","load_dotenv()\n","openai.organization = os.getenv(\"OPENAI_ORG\")\n","openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","# Point to your self-hosted redis server OR try a free redis cloud server from https://redis.com/try-free/\n","REDIS_URL = os.getenv(\"REDIS_URL\")\n","\n","try:\n","    openai.Model.list()\n","except:\n","    print(\"OpenAI authentication error\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685747123943,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"iMTC5i-bMLwD","outputId":"a2b8a031-73c6-4ed3-e260-c427a0222b53"},"outputs":[],"source":["# Since we have some long lines, let's make sure output is line wrapped\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qYS1Vk-xNRBj"},"source":["## Download the data\n","\n","Data already download in ~/data folder."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"executionInfo":{"elapsed":46872,"status":"ok","timestamp":1685747170808,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"Wxl__9HQGoML","outputId":"84ba3fc8-5467-46d8-83f0-47dcab1c80bd"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# import gdown\n","# gdown.download_folder(\"https://drive.google.com/drive/folders/1FCuU2j8yI7hXsZL8Ls_fgJwUn7-Dx_VV?usp=sharing\", quiet=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ff4PODEINaXH"},"source":["### Raw Messages & Embeddings"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1685747171544,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"Gb3WVLtRJpsN","outputId":"f1cfe61a-b8c3-43a0-cf18-9e868b1da0e3"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>__Source</th>\n","      <th>User_ID</th>\n","      <th>Channel_Name</th>\n","      <th>Message_Timestamp</th>\n","      <th>Thread_Timstamp</th>\n","      <th>Channel_ID</th>\n","      <th>__Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>threads</td>\n","      <td>U01T78HPG3H</td>\n","      <td>computer-vision</td>\n","      <td>2023-05-18 11:36:44.001949 UTC</td>\n","      <td>2023-05-18 11:30:13.159979 UTC</td>\n","      <td>C026ED0PZEZ</td>\n","      <td>Both &lt;https://roboflow.github.io/supervision/q...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>threads</td>\n","      <td>U01T78HPG3H</td>\n","      <td>computer-vision</td>\n","      <td>2023-05-18 11:30:13.159979 UTC</td>\n","      <td>2023-05-18 11:30:13.159979 UTC</td>\n","      <td>C026ED0PZEZ</td>\n","      <td>Would love a package for suggesting and then i...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>threads</td>\n","      <td>U04QRD69H8Q</td>\n","      <td>computer-vision</td>\n","      <td>2023-05-18 10:56:33.313369 UTC</td>\n","      <td>2023-05-17 12:35:14.522419 UTC</td>\n","      <td>C026ED0PZEZ</td>\n","      <td>&lt;@U056Q4V4FFC&gt; how about this dataset (for the...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>threads</td>\n","      <td>U01J0NVNE1G</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 05:21:04.346819 UTC</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I always oppose the counterargument, why do yo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>threads</td>\n","      <td>U01J0NVNE1G</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 05:19:51.718379 UTC</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I find MLflow more convenient to use. Here are...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  __Source      User_ID              Channel_Name  \\\n","0  threads  U01T78HPG3H           computer-vision   \n","1  threads  U01T78HPG3H           computer-vision   \n","2  threads  U04QRD69H8Q           computer-vision   \n","3  threads  U01J0NVNE1G  mlops-questions-answered   \n","4  threads  U01J0NVNE1G  mlops-questions-answered   \n","\n","                Message_Timestamp                 Thread_Timstamp  \\\n","0  2023-05-18 11:36:44.001949 UTC  2023-05-18 11:30:13.159979 UTC   \n","1  2023-05-18 11:30:13.159979 UTC  2023-05-18 11:30:13.159979 UTC   \n","2  2023-05-18 10:56:33.313369 UTC  2023-05-17 12:35:14.522419 UTC   \n","3  2023-05-18 05:21:04.346819 UTC  2023-05-18 01:08:57.948139 UTC   \n","4  2023-05-18 05:19:51.718379 UTC  2023-05-18 01:08:57.948139 UTC   \n","\n","    Channel_ID                                             __Text  \n","0  C026ED0PZEZ  Both <https://roboflow.github.io/supervision/q...  \n","1  C026ED0PZEZ  Would love a package for suggesting and then i...  \n","2  C026ED0PZEZ  <@U056Q4V4FFC> how about this dataset (for the...  \n","3  C015J2Y9RLM  I always oppose the counterargument, why do yo...  \n","4  C015J2Y9RLM  I find MLflow more convenient to use. Here are...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Raw messages\n","messages_df = pd.read_csv(\"../data/messages.csv\")\n","messages_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1685747171545,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"t2DI96m7OOjK","outputId":"18156c77-12f3-40f0-a03a-677acd5bb672"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>__Source</th>\n","      <th>User_ID</th>\n","      <th>Channel_Name</th>\n","      <th>Message_Timestamp</th>\n","      <th>Thread_Timstamp</th>\n","      <th>Channel_ID</th>\n","      <th>__Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>threads</td>\n","      <td>U01J0NVNE1G</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 05:21:04.346819 UTC</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I always oppose the counterargument, why do yo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>threads</td>\n","      <td>U01J0NVNE1G</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 05:19:51.718379 UTC</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I find MLflow more convenient to use. Here are...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>threads</td>\n","      <td>U01CRVDS4NA</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 05:14:52.514189 UTC</td>\n","      <td>2023-05-16 23:22:04.332479 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I just built some demos with it. The developer...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>messages</td>\n","      <td>U01VCA57PD0</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>2023-05-18 01:08:57.948139 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>These days I'm feeling very tempted to roll my...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>messages</td>\n","      <td>U015BH45ZK6</td>\n","      <td>mlops-questions-answered</td>\n","      <td>2023-05-17 14:56:59.775629 UTC</td>\n","      <td>2023-05-17 14:56:59.775629 UTC</td>\n","      <td>C015J2Y9RLM</td>\n","      <td>I ran into a problem downloading files from Az...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    __Source      User_ID              Channel_Name  \\\n","3    threads  U01J0NVNE1G  mlops-questions-answered   \n","4    threads  U01J0NVNE1G  mlops-questions-answered   \n","5    threads  U01CRVDS4NA  mlops-questions-answered   \n","7   messages  U01VCA57PD0  mlops-questions-answered   \n","21  messages  U015BH45ZK6  mlops-questions-answered   \n","\n","                 Message_Timestamp                 Thread_Timstamp  \\\n","3   2023-05-18 05:21:04.346819 UTC  2023-05-18 01:08:57.948139 UTC   \n","4   2023-05-18 05:19:51.718379 UTC  2023-05-18 01:08:57.948139 UTC   \n","5   2023-05-18 05:14:52.514189 UTC  2023-05-16 23:22:04.332479 UTC   \n","7   2023-05-18 01:08:57.948139 UTC  2023-05-18 01:08:57.948139 UTC   \n","21  2023-05-17 14:56:59.775629 UTC  2023-05-17 14:56:59.775629 UTC   \n","\n","     Channel_ID                                             __Text  \n","3   C015J2Y9RLM  I always oppose the counterargument, why do yo...  \n","4   C015J2Y9RLM  I find MLflow more convenient to use. Here are...  \n","5   C015J2Y9RLM  I just built some demos with it. The developer...  \n","7   C015J2Y9RLM  These days I'm feeling very tempted to roll my...  \n","21  C015J2Y9RLM  I ran into a problem downloading files from Az...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Get all data for specific channel\n","df_mlops_questions_answered = messages_df[messages_df[\"Channel_Name\"]==\"mlops-questions-answered\"]\n","df_mlops_questions_answered.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685747171546,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"NhceGvxtOUff","outputId":"c87dade7-1705-4146-f0ed-38fa41bbaa4e"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Number of conversations: 2086\n"]}],"source":["# Get the count of conversations\n","print(f\"Number of conversations: {len(df_mlops_questions_answered.groupby(['Thread_Timstamp']))}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1685747172095,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"tBg-2ZBvOZ8p","outputId":"d1754640-0378-4998-ea4a-f1f9da8f301a"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Channel: mlops-questions-answered / conversation 2020-06-19 20:44:41.0065 UTC </h2><hr/>\n","<table>\n","<tr><td>U011NTHUKEF</td><td>We use a lot of TPOT library. The main advantage is that it does hyperparameter tunning and also deals with preprocessing steps as well... downside is that it works mostly with scikit-learn algorithms. But there is a small hack you can do to make it work with many algorithms... make a new algo class that inherits both from sklearn BaseEstimators and your algo (Catboost, for example, or Pygams)</td></tr><tr><td>U015CHWG25B</td><td>Aha, I’ve heard that it’s really good!</td></tr><tr><td>U0150LZ578X</td><td>For anyone already using other Kubeflow components, Katib is relatively easy to work with <https://github.com/kubeflow/katib>\n","\n","It parallelizes trials across k8s pods and provides a web UI to visualize the hyperparameter space for each training history!</td></tr><tr><td>U015CHWG25B</td><td><@U013CL3GTB3> Thank you! Looks like this is an entire ML platform rather than just a hyperparameter tuning tool :thinking_face: </td></tr><tr><td>U013CL3GTB3</td><td>Polyaxon is another good one! <https://medium.com/polyaxon/polyaxon-0-0-2-23964df6ef7e|https://medium.com/polyaxon/polyaxon-0-0-2-23964df6ef7e></td></tr><tr><td>U015CHWG25B</td><td>At some point we reviewed about 15 hyperparameter tuning tools in order to choose one that answers our needs. We stopped at NNI from Microsoft (<https://github.com/microsoft/Nni|https://github.com/microsoft/Nni>). This tool is designed to run hyperparameter tuning in several parallel jobs. Unlike many other tools, it supports a lot of different algorithms of hyperparameter tuning. It has a decent UI. Plus, it's OSS from Microsoft :) Do you know other tools which answer our criteria?</td></tr><tr><td>U015CHWG25B</td><td>I want to run hyperparameter tuning on several machines in parallel and track the process in Web UI. What is the best tool for that?</td></tr></table>\n","<hr/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Channel: mlops-questions-answered / conversation 2020-06-19 20:45:07.0067 UTC </h2><hr/>\n","<table>\n","<tr><td>U011NTHUKEF</td><td>AirFlow is a great tool. We use it a lot. When we work on a AWS environment, we use AWS stepfunctions which has its own Data Science SDK. Works like a charm!</td></tr><tr><td>U015CHWG25B</td><td>This is a clear usecase for pipeline tools. There are plenty of them out there, providing various features and UI capabilities. Currently, for our projects we use AirFlow. Which pipeline tools do you prefer and why?</td></tr><tr><td>U015CHWG25B</td><td>OK, I have a model of decent quality. Now I want to automate daily collecting data, retraining the model, and redeployment. How do I do that?</td></tr></table>\n","<hr/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Channel: mlops-questions-answered / conversation 2020-06-24 00:53:14.0183 UTC </h2><hr/>\n","<table>\n","<tr><td>U015CHWG25B</td><td>Sure, thank you! :pray:</td></tr><tr><td>U016A3RAL5N</td><td><@U015CHWG25B> thanks for including us in your list. If you need any support lest us know</td></tr></table>\n","<hr/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Channel: mlops-questions-answered / conversation 2020-06-24 17:51:24.026 UTC </h2><hr/>\n","<table>\n","<tr><td>UV92GMLF4</td><td><@U014Z58NT25> Sorry for the late reply:\n","\n","1. It's determined by marketing. The thing that we do it's just to highlight who is in the border. \n","2. We used only numericals in that time, but OHE work fine also. \n","3. At the that time, we didn't used any specific tool. I think today it's doable to do in matplotlib if you transform in 2D array those records from the cluster. </td></tr><tr><td>U012YQULW4X</td><td>you seem to treat it as a technical problem (automate the encoding). This issue points you to a potentially significant change in the input data so you want to notice and manually investigate. For detection, you can check tools for data monitoring like <https://github.com/great-expectations/great_expectations> or tensorflow data monitoring. They should be able to check feature cardinality (number of segments).\n","\n","Segments: Do I understand you correctly that the marketing team changed the logic for the segment variable e.g. an prior \"A\" customer might now be a \"B\" customer?\n","Case A: Feature is not important. Fix encoding bug and move on.\n","Case B: The feature is important but not a lot of customers migrated their segment. Fix encoding bug and move on.\n","Case C:  Feature is important and the logic is very different or many people migrated. This is possibly a breaking change in the data and you need a migration plan (discard training data with old logic, not use the feature for the migration duration etc):\n","\n","you mention: \"everytime this happens\". That should not happen regularly. If it does, you have to stop using the feature/understand better what they do and what it means. Constantly changing data definitions make the feature dangerous to use.</td></tr><tr><td>UP3T8K9M5</td><td>maybe <@U012YQULW4X> can also help as she has some experience with monitoring and will be talking to us about it during this week’s meetup</td></tr><tr><td>U014Z58NT25</td><td>And thanks for the answers guys! It really helps even just to talk about the problem</td></tr><tr><td>U014Z58NT25</td><td><@UV92GMLF4> the clusters were determined by the marketing in the sense of \"good cluster/bad cluster\" or they even gave you a centroid for that? Could you use anything but numericals for KMeans? Essentially I know you can't, for you have to deal with distances. But someone has once told me you can sometimes work with binary/ordinal using KMeans as well.\n","Also, did you have anything (besides your own code and CLI) for monitoring those jumpers? Any specific tools or dashboards</td></tr><tr><td>U014Z58NT25</td><td><@U013CL3GTB3> What happens is that as of now I have, say, 5 customer segments. On the data that I extract tomorrow, I might have 6, cause this customer segmentation is something still being created by the business, and I won't know it in advance.\n","\n","I can extract some meaning from these classes and put them into an ascending order (of integers, for example). And yes, I wanna use them for my recsys at the end of the day.\n","\n","EDIT: typo</td></tr><tr><td>UP3T8K9M5</td><td><@U0156CADGJG> might have something to say about this too</td></tr><tr><td>UV92GMLF4</td><td>Those guys I removed from the clusters and put them in another cluster (determined by marketing).\n","\n","Was totally primitive and I was using the KMeans, but the general idea it's that.</td></tr><tr><td>UV92GMLF4</td><td>In pink I got the \"jumpers\".</td></tr><tr><td>UV92GMLF4</td><td>Those are the clusters (just for simplification)</td></tr><tr><td>UV92GMLF4</td><td>Hey Murilo! What's up!\n","\n","I worked with that in a long time ago (2015) so maybe my answer it's outdated.\n","\n","In my case, I had fixed clusters (due to the fact those clusters where determined by Marketing); so the only thing that I monitor in this case was 1) the Centroid (to check if there's some changes in the centroid (drift) and 2) in the instances that I call \"jumpers\", i.e. records that shifted to some point of clusters boarders.\n","\n","Like this.</td></tr><tr><td>U013CL3GTB3</td><td>• Are these segment features known in advance? Or are they being generated on the fly? Are they different every time? Is it possible to know the values in advance? \n","• can you clarify what you mean by “encoding” ? Are you given a string and want to turn it into a int or float? I’m assuming you use it as a feature for the recommender system.</td></tr><tr><td>U014Z58NT25</td><td>Hey guys! In the matter of concept drifting, I have a clustering algorithm (that is mainly a recommendation system for our business) that takes into account a categorical variable called \"Customer Segment\", that is translated in an ordinal fashion - i.e. \"better\" clients go to a higher number. Due to this new scenario we're going through, some new segments showed up and everytime this happens I have to manually determine how to encode them. Is there a way to automate this? Or even tools to identify when it happens would already help me out a lot. Thanks!</td></tr></table>\n","<hr/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h2>Channel: mlops-questions-answered / conversation 2020-06-27 19:23:54.0496 UTC </h2><hr/>\n","<table>\n","<tr><td>U016FCTSDGS</td><td>Few answers from our experience:\n","1. We keep Airflow stack separate from DAG Projects. We also have a single repo per DAG project that gets auto-named and deployed in such a way that Airflow picks it up to sync in the DAG folder.  We also have a DAG that runs frequently to scan and ingest the new DAG(s) from the DAG Projects.\n","2. If the question is long running tasks we spin up AWS Batch or AWS EMR depending on the need. If the question is how to interject DAG updates if a long running task is executing then no solution here other than on the next run the task(s)/DAG will get updated.\n","3. Did not know about the *`PythonVirtualenvOperator`*  or would have used it. We just installed custom python packages to a temp directory and added to Python path, then deleted the folder in the last task for cleanup.\n","4. Haven't used Docker Operator.\n","5. Let me know if you find one!</td></tr><tr><td>U015CHWG25B</td><td>I can’t answer all your questions one by one, but I can share the basics of our approach.\n","• Regarding deployment: We use Git to deploy DAGs. In our case, we have a dedicated repo for DAGs, and AirFlow installation polls this repo every minute and redeploys DAGs if necessary. Thus your DAGs are separated from your Airflow service.\n","• Regarding tasks: We chose to make our own operator (as we have an ML platform with some specificity) based on DockerOperator. Containerisation of each step gives you way more freedom, especially in complex pipelines when you may need even different versions of CUDA for different steps. \n","</td></tr><tr><td>U013K7876BF</td><td>thanks for the responses <@U013CL3GTB3> and <@U0158N59C8H>\n","Flux sounds great, but I’m going need to work with a push model (we have some constraints in our system -  we can’t access the repo from the cloud)\n","If i’m pushing the scheduler just to kick off a `KubernetesPodOperator`, Airflow feels very similar to Argo.</td></tr><tr><td>UP3T8K9M5</td><td>hmmm yeah i also would like to know this.</td></tr><tr><td>U0158N59C8H</td><td>Nope. I mean I use argo quite a bit via kubeflow pipelines. I used airflow too. Just wanting to get more opinions on the diff solutions.\n","\n","Esp Argo vs Tekton. As they are very closely related.</td></tr><tr><td>UP3T8K9M5</td><td>I also remember <@U0158N59C8H> was asking about Argo a while back. Were you able to get any info that could help us? </td></tr><tr><td>U013CL3GTB3</td><td>1. I would launch the task in a pod using the kubernetesPodOperator. And for pushing your dags, use gitops! You can point your airflow deployment to a git branch. This requires flux and some other stuff to be in place.\n","2. Not sure what you mean by long running airflow service? Are you saying how to update a task independent of airflow itself like the web server or scheduler?  - if you're launching your tasks in pods you leave the execution logic to the docker container running in the pod and the orchestration logic to airflow. I would also recommend using the KubernetesExecutor alongside the KubernetesPodOperator. \n","3. The kubernetesPodOperator also solves this problem as you have complete control over the runtime environment, resources, etc - the container has everything it needs to run the code\n","4. Haven’t used the docker operator but It sounds like it’s more general than just python code. And will allow you to containerize your code which deals with the dependency issue.\n","5. My company BenevolentAI Is about to publish an article about some lessons we learned using airflow so I’ll post it to the community when it’s out.\n","Also me and <@UP3T8K9M5> are planning on doing a coffee session on pipelines - and airflow will be one of them. You mentioned Argo, that’s also another good option as it’s native to kubernetes which is nice and also has other benefits.</td></tr><tr><td>U013K7876BF</td><td>Hey everyone\n","We’re looking for productionize some data engineering pipelines but we really want this run on Airflow and have a proper CI/CD pipeline.\n","I have a couple of questions, and I’m hoping some of you may have experience and knowledge about these topics.\n","1. Would you deploy the Airflow task along with the Airflow service itself? How else would you push your DAGs from source control into the dags folder of Airflow?\n","2. Is there an elegant way to update the tasks separately from a long running Airflow service?\n","3. Given that not all tasks have the same python dependencies, how does dependency management work? Do you install all of the dependencies with Airflow itself, or do you use `PythonVirtualenvOperator` to create a virtual env for each step in the dag?\n","4. How difficult is monitoring and debugging when using `airflow.operators.docker_operator`? we can package each step in our data engineering pipeline in an .py file with a `main` and create a container. Does it have clear benefits over the `PythonVirtualenvOperator?`\n","5. If there are good resource on airflow deployment strategies, I’d love to read more. I was looking at Argo <https://github.com/argoproj/argo> and if help have experience with Argo vs Airflow I’d love to hear that too.\n","Thanks!</td></tr></table>\n","<hr/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Iterate through conversations\n","NUM_CONVERSATIONS_TO_PRINT=5\n","current_conversation=0\n","for (channel_name, thread_id), conv in df_mlops_questions_answered.groupby(['Channel_Name', 'Thread_Timstamp']):\n","  html = f\"<h2>Channel: {channel_name} / conversation {thread_id} </h2><hr/>\\n<table>\\n\"\n","  for index, row in conv.iterrows():\n","    html+= f'<tr><td>{row[\"User_ID\"]}</td><td>{row[\"__Text\"]}</td></tr>'\n","  html += f\"</table>\\n<hr/><br/>\"\n","  display(HTML(html))\n","  current_conversation+=1\n","  if current_conversation>=NUM_CONVERSATIONS_TO_PRINT:\n","    break"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":21743,"status":"ok","timestamp":1685747193833,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"8tKT42ffOZ57","outputId":"8e1a87fd-4d60-41da-cd4b-6abf1adcd1df"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>message_id</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2020-03-17 22:52:45.0174 UTC</td>\n","      <td>[-0.043248970061540604, -0.007734753657132387,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2020-03-17 22:58:47.0175 UTC</td>\n","      <td>[-0.03162849321961403, 0.0018300998490303755, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2020-03-18 06:39:01.0184 UTC</td>\n","      <td>[-0.021394122391939163, -0.013377929106354713,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2020-03-18 06:40:19.0195 UTC</td>\n","      <td>[0.014829986728727818, -0.01779334992170334, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2020-03-18 06:40:58.0198 UTC</td>\n","      <td>[0.013494313694536686, -0.009085348807275295, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                    message_id  \\\n","0           0  2020-03-17 22:52:45.0174 UTC   \n","1           1  2020-03-17 22:58:47.0175 UTC   \n","2           2  2020-03-18 06:39:01.0184 UTC   \n","3           3  2020-03-18 06:40:19.0195 UTC   \n","4           4  2020-03-18 06:40:58.0198 UTC   \n","\n","                                           embedding  \n","0  [-0.043248970061540604, -0.007734753657132387,...  \n","1  [-0.03162849321961403, 0.0018300998490303755, ...  \n","2  [-0.021394122391939163, -0.013377929106354713,...  \n","3  [0.014829986728727818, -0.01779334992170334, 0...  \n","4  [0.013494313694536686, -0.009085348807275295, ...  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Embeddings data for each message (rahul is generating this rn.) \n","message_embeddings_df = pd.read_csv(\"../data/messages-embeddings-ada-002.csv\")\n","message_embeddings_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K1W_efjwNiut"},"source":["### Conversations & Embeddings"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1685727873622,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"xTGDU-5RKwLz","outputId":"173f13f0-e747-441b-bade-25338f69f4fc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>channel_name</th>\n","      <th>thread_id</th>\n","      <th>chat_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>africa</td>\n","      <td>2022-03-22 19:42:06.219769 UTC</td>\n","      <td>U024WRAA0D9: Hello fellow MLOpsers in Africa :...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>africa</td>\n","      <td>2022-03-24 08:14:33.140029 UTC</td>\n","      <td>U024WRAA0D9: What should our next steps be (fo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>africa</td>\n","      <td>2022-03-28 11:57:42.840049 UTC</td>\n","      <td>U024WRAA0D9: What’s everyone’s timezone?U024WR...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>africa</td>\n","      <td>2022-04-12 14:36:00.144498 UTC</td>\n","      <td>U03142DQP6Z: Please can we make it later in th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>africa</td>\n","      <td>2022-04-19 10:24:57.455849 UTC</td>\n","      <td>U024WRAA0D9: Hello &lt;#C037GTG932B|africa&gt;, I on...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0 channel_name                       thread_id  \\\n","0           0       africa  2022-03-22 19:42:06.219769 UTC   \n","1           1       africa  2022-03-24 08:14:33.140029 UTC   \n","2           2       africa  2022-03-28 11:57:42.840049 UTC   \n","3           3       africa  2022-04-12 14:36:00.144498 UTC   \n","4           4       africa  2022-04-19 10:24:57.455849 UTC   \n","\n","                                           chat_text  \n","0  U024WRAA0D9: Hello fellow MLOpsers in Africa :...  \n","1  U024WRAA0D9: What should our next steps be (fo...  \n","2  U024WRAA0D9: What’s everyone’s timezone?U024WR...  \n","3  U03142DQP6Z: Please can we make it later in th...  \n","4  U024WRAA0D9: Hello <#C037GTG932B|africa>, I on...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Each conversation grouped into a single thread_id\n","chats_df = pd.read_csv(\"../data/chats.csv\")\n","chats_df.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":3013,"status":"ok","timestamp":1685728015964,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"wHQUy394K_5s","outputId":"36a2d99d-8809-456f-b311-5d2959acdeae"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>thread_id</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2022-03-22 19:42:06.219769 UTC</td>\n","      <td>[0.0036350293084979057, -0.01264416053891182, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2022-03-24 08:14:33.140029 UTC</td>\n","      <td>[-0.002360287122428417, -0.04199115186929703, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2022-03-28 11:57:42.840049 UTC</td>\n","      <td>[0.017543835565447807, 0.0032007887493819, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2022-04-12 14:36:00.144498 UTC</td>\n","      <td>[-0.001173610333353281, -0.014446504414081573,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2022-04-19 10:24:57.455849 UTC</td>\n","      <td>[-0.0025763490702956915, -0.02925489842891693,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                       thread_id  \\\n","0           0  2022-03-22 19:42:06.219769 UTC   \n","1           1  2022-03-24 08:14:33.140029 UTC   \n","2           2  2022-03-28 11:57:42.840049 UTC   \n","3           3  2022-04-12 14:36:00.144498 UTC   \n","4           4  2022-04-19 10:24:57.455849 UTC   \n","\n","                                           embedding  \n","0  [0.0036350293084979057, -0.01264416053891182, ...  \n","1  [-0.002360287122428417, -0.04199115186929703, ...  \n","2  [0.017543835565447807, 0.0032007887493819, 0.0...  \n","3  [-0.001173610333353281, -0.014446504414081573,...  \n","4  [-0.0025763490702956915, -0.02925489842891693,...  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# The embedding for each conversation with its thread_id (Note: not all embeddings were generated for the chat text)\n","embeddings_df = pd.read_csv(\"../data/chats-embeddings-ada-002.csv\")\n","embeddings_df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8575,"status":"ok","timestamp":1685295657664,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"4wAmWK4dMg_8","outputId":"efb9ea74-2efc-4b78-b90f-ef07c937bd62"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Creating temporary chats index: 9719it [00:01, 6558.30it/s]\n","Collecting chats and embeddings: 9713it [00:13, 731.04it/s]\n"]}],"source":["# Get your embeddings data together.\n","# Create a temp index of the chats\n","chats_index = {}\n","for _, row in tqdm(chats_df.iterrows(), desc=\"Creating temporary chats index\"):\n","  chats_index[row['thread_id']] = row['chat_text']\n","\n","# Link the chats and embeddings together\n","embeddings = []\n","VECTOR_SIZE = None\n","for _, row in tqdm(embeddings_df.iterrows(), desc=\"Collecting chats and embeddings\"):\n","  embedding = json.loads(row['embedding'])\n","  embeddings.append({\"thread_id\": row['thread_id'], \"embedding\":  embedding})\n","  if not VECTOR_SIZE:\n","    VECTOR_SIZE = len(embedding)\n","  else:\n","    assert VECTOR_SIZE==len(embedding)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E_MIq52NMbyT"},"source":["# Create your vector database\n","Using https://redis-py.readthedocs.io/en/stable/examples/search_vector_similarity_examples.html"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"dtAsNPAEL-fV"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["r = redis.StrictRedis.from_url(REDIS_URL)\n","r.ping()\n","\n","INDEX_NAME = \"vector_index\"                              # Vector Index Name\n","DOC_PREFIX = \"DOC:\"                                      # RediSearch Key Prefix for the Index\n","assert VECTOR_SIZE\n","\n","def create_index(vector_dimensions: int=VECTOR_SIZE):\n","    try:\n","        # check to see if index exists\n","        r.ft(INDEX_NAME).info()\n","        print(\"Index already exists!\")\n","    except:\n","        # schema - we have two fields in our object - thread_id, and embedding\n","        schema = (\n","            VectorField(\"vector\",                  # Vector Field Name\n","                \"FLAT\", {                          # Vector Index Type: FLAT or HNSW\n","                    \"TYPE\": \"FLOAT32\",             # FLOAT32 or FLOAT64\n","                    \"DIM\": vector_dimensions,      # Number of Vector Dimensions\n","                    \"DISTANCE_METRIC\": \"COSINE\",   # Vector Search Distance Metric\n","                }\n","            ),\n","        )\n","\n","        # index Definition\n","        definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)\n","\n","        # create Index\n","        r.ft(INDEX_NAME).create_index(fields=schema, definition=definition)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"_4VCtBrfOXT_"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Index already exists!\n"]}],"source":["# Create the empty index\n","create_index(vector_dimensions=VECTOR_SIZE)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6071,"status":"ok","timestamp":1685295802866,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"0MfzhP7KP7PT","outputId":"beb0ca60-2a51-4aaf-c12b-e32e46827daf"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9713/9713 [00:01<00:00, 4947.77it/s]\n"]}],"source":["# bulk insert data\n","pipe = r.pipeline()\n","for row in tqdm(embeddings):\n","    # define key\n","    key = f\"{DOC_PREFIX}{row['thread_id']}\"\n","    # HSET\n","    pipe.hset(key, mapping={\"vector\": np.array(row['embedding']).astype(np.float32).tobytes()})\n","res = pipe.execute()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"pS_lC8k6SJTQ"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def search_index(vector, vector_dimensions: int = VECTOR_SIZE):  \n","  query = (\n","    Query(\"*=>[KNN 2 @vector $vec as score]\")\n","     .sort_by(\"score\")\n","     .return_fields(\"id\", \"score\")\n","     .paging(0, 3)\n","     .dialect(2)\n","  )\n","\n","  query_params = {\n","      \"vec\": vector\n","  }\n","  return r.ft(INDEX_NAME).search(query, query_params).docs"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jI_XJYqwY0pP"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# test with an existing document, that that document is returned\n","row = embeddings[2539]\n","docs = search_index(np.array(row['embedding']).astype(np.float32).tobytes())\n","assert f\"{DOC_PREFIX}{row['thread_id']}\"==docs[0].id, \"Document does not match\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nuiB3wZHbb2_"},"source":["# Example Q&A with OpenAI"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Ij-IEfqlbevd"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def get_embedding(text):  \n","  # Use the same embedding generator as what was used on the data!!!\n","  response = openai.Embedding.create(\n","    model=\"text-embedding-ada-002\",\n","    input=text\n","  )\n","  return response.data[0].embedding\n","\n","def summarize(chat_text):\n","  # Summarize conversations since individually they are long and go over 8k limit\n","  prompt = \"Summarize the following conversation on the MLOps.community slack channel. Do not use the usernames in the summary. ```\" + chat_text + \"```\"\n","  completion = openai.ChatCompletion.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[\n","      {\"role\": \"user\", \"content\": prompt}\n","    ]\n","  )\n","  return completion.choices[0].message.content\n","  \n","\n","def extract_answer(chat_texts, question):  \n","  # Combine the summaries into a prompt and use SotA GPT-4 to answer.\n","  prompt = \"Use the following summaries of conversations on the MLOps.community slack channel backtics to generate an answer for the user question.\"\n","  for i, chat_text in enumerate(chat_texts):\n","    print(f\"Getting summary for conversation {i+1}\")\n","    prompt += f\"\\nConversation {i+1} Summary:\\n```\\n{summarize(chat_text)}```\"\n","\n","  if not question.endswith(\"?\"):\n","    question = question + \"?\"\n","  prompt+= f\"\\nQuestion: {question}\"\n","  print(f\"Getting answer for the question.\")\n","  completion = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","      {\"role\": \"user\", \"content\": prompt}\n","    ]\n","  )\n","  content = completion.choices[0].message.content\n","  return content\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"4FTUR7aSbiR4"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def get_answer(question):\n","  # Get answer to the question by finding the three conversations that are nearest \n","  # to the question and then using them to generate the answer.\n","  print(f\"Searching documents nearest to the question.\")\n","  search_vector = np.array(get_embedding(question)).astype(np.float32).tobytes()\n","  docs = search_index(search_vector)\n","  # Take the top three answers, and use ChatGPT to form the answer to give the user.\n","  chat_texts = []\n","  for doc in docs:\n","    chat_text = chats_index[doc.id[len(DOC_PREFIX):]]\n","    chat_texts.append(chat_text)\n","  if len(chat_texts)>3:\n","    chat_texts[:3]\n","  return extract_answer(chat_texts, question)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":33720,"status":"ok","timestamp":1685297728941,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"bZDd3d5lcSSz","outputId":"e333a17a-e5c1-4675-bf97-9fe1b1f51fc9"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Searching documents nearest to the question.\n","Getting summary for conversation 1\n","Getting summary for conversation 2\n","Getting answer for the question.\n","\n","\n","Question: What are some good ways to deploy models on Kubernetes?\n","Answer: Some good ways to deploy models on Kubernetes include using KServe, Seldon, TensorFlow Serving, and Torchserve. These tools allow you to effectively serve and manage ML models in production. You can also use ONNX Runtime for significant speedups by converting your model checkpoints to ONNX format. Additionally, data orchestration products like Pachyderm, Algorithmia, and Iguazio can be utilized for custom preprocessing and data lineage. When deploying models, it's also a good idea to separate the API/web service from the model inference service for better scalability and use tools like MLflow for user-friendly management.\n"]}],"source":["question=\"What are some good ways to deploy models on Kubernetes?\"\n","answer = get_answer(question)\n","print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":56850,"status":"ok","timestamp":1685297830627,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"Ub0bI4roeUHK","outputId":"134df44e-5e10-486b-f56a-a63507aa3940"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Searching documents nearest to the question.\n","Getting summary for conversation 1\n","Getting summary for conversation 2\n","Getting answer for the question.\n","\n","\n","Question: How can I structure a good Data Science team?\n","Answer: A good Data Science team structure can be achieved by adopting the following strategies:\n","\n","1. Embed data scientists in product teams: This helps in close collaboration with the product team and enables them to understand the business requirements better.\n","\n","2. Educate stakeholders: Ensure that stakeholders understand the unpredictable nature of project duration, model accuracy, and business value in data science projects.\n","\n","3. Iterate and explore: Adopt an iterative and exploratory approach to data science projects, allowing the team to refine their models and solutions over time.\n","\n","4. Review KPIs in terms of ROI: Regularly review key performance indicators with a focus on return on investment to ensure that data science projects align with business goals.\n","\n","5. Centralize ML platform and Data Science center of competence: Implement a matrix structure that combines both centralized and decentralized elements, providing a central hub for expertise and resources while allowing individual teams the autonomy to work on their specific projects.\n","\n","6. Collaborate with ML Engineers and Infrastructure Software Engineers: Depending on the project type, you may need to work closely with ML Engineers or Infrastructure Software Engineers to bridge the gap between ML development and production.\n","\n","It is essential to understand that the ideal structure for your Data Science team may vary depending on the organization's specific needs and goals. Therefore, it is crucial to continuously evaluate your team's structure and make adjustments as needed for optimal results.\n"]}],"source":["question=\"How can I structure a good Data Science team?\"\n","answer = get_answer(question)\n","print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":34659,"status":"ok","timestamp":1685297965833,"user":{"displayName":"Rahul Parundekar","userId":"17922132101761181839"},"user_tz":420},"id":"Hx4hElL0kzlV","outputId":"d06244b1-79cc-4ae3-b36e-79a6db1bcc99"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Searching documents nearest to the question.\n","Getting summary for conversation 1\n","Getting summary for conversation 2\n","Getting answer for the question.\n","\n","\n","Question: What is the best way to train models for tabular data?\n","Answer: The best way to train models for tabular data is often to use boosted decision trees, as they tend to be on par with or outperform neural networks for this type of data. They are also quicker to train, making them suitable for fast iterations. By using smart sampling and limited hyperparameter tuning, trees can achieve high accuracy. However, if you need to blend tabular data with other data types like images or text, using a library like pytorch-widedeep can be effective.\n"]}],"source":["question=\"What is the best way to train models for tabular data?\"\n","answer = get_answer(question)\n","print(f\"\\n\\nQuestion: {question}\\nAnswer: {answer}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-tnVPN3laIl"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM+PK0O8A9lJRy7IacgueMf","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
